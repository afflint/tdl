#### Studi Umanistici

# Tecnologie dei dati e del linguaggio

## Docente Prof. Alfio Ferrara

### Assistente Dott. Sergio Picascia

## Riferimenti bibliografici

#### Introduzione al natural language processing

> Luhn, H. P. (1957). A statistical approach to mechanized encoding and searching of literary information. IBM Journal of research and development, 1(4), 309-317.

> Sparck Jones, Karen. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation 28.1 (1972): 11-21.

> Salton, Gerard and McGill, Michael J. Introduction to Modern Information Retrieval. McGraw-Hill, 1983

> Schütze, H., Manning, C. D., & Raghavan, P. (2008). Introduction to information retrieval (Vol. 39, pp. 234-265). Cambridge: Cambridge University Press.

#### Language models

> Li, Hang. "Language models: past, present, and future." Communications of the ACM 65.7 (2022): 56-63

> Johnson, Mark. "How the statistical revolution changes (computational) linguistics." Proceedings of the EACL 2009 workshop on the interaction between linguistics and computational linguistics: Virtuous, vicious or vacuous?. 2009.

#### Reti neurali per i language models

> Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic
> language model. Journal of machine learning research, 3(Feb), 1137-1155

> Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. "Learning representations by back-propagating errors." nature 323.6088 (1986): 533-536.

> Schmidhuber, Jürgen. "Deep learning in neural networks: An overview." Neural networks 61 (2015): 85-117.

> Powers, David. "Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation." Journal of Machine Learning Technologies 2.1 (2011): 37-63.

#### Word Embedding

> Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.

#### Recurrent Neural Networks

> Pascanu, R., Gulcehre, C., Cho, K., & Bengio, Y. (2013). How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026.

#### Transformers

> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

> Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019, June). Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers) (pp. 4171-4186).

> Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. (GPT, online at [https://www.mikecaptain.com/resources/pdf/GPT-1.pdf](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf))

#### Immagini e Computer Vision

> Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.

> Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PmLR.

#### Prompt Engineering

> Schulhoff, S., Ilie, M., Balepur, N., Kahadze, K., Liu, A., Si, C., ... & Resnik, P. (2024). The prompt report: A systematic survey of prompting techniques. arXiv preprint arXiv:2406.06608, 5.

#### Explainability

> Zhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., ... & Du, M. (2024). Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology, 15(2), 1-38.

#### Bias e Stereotipi

> Naous, T., Ryan, M. J., Ritter, A., & Xu, W. (2023). Having beer after prayer? measuring cultural bias in large language models. arXiv preprint arXiv:2305.14456.

> Li, C., Chen, M., Wang, J., Sitaram, S., & Xie, X. (2024). Culturellm: Incorporating cultural differences into large language models. Advances in Neural Information Processing Systems, 37, 84799-84838.